# robust_detect_RPI_LiveDet.py
# Drop-in replacement: keeps smooth display & camera settings, improves detection robustness.
# Run: python robust_detect_RPI_LiveDet.py

import time
import threading
import cv2
import torch
from ultralytics import YOLO
from picamera2 import Picamera2
from libcamera import controls
import numpy as np

# -------------------------- PyTorch fix --------------------------
orig_torch_load = torch.load
def torch_load_wrapper(*args, **kwargs):
    kwargs["weights_only"] = False
    return orig_torch_load(*args, **kwargs)
torch.load = torch_load_wrapper

# -------------------------- Tunable knobs (robust defaults) --------------------------
CAPTURE_SIZE = (1280, 720)   # (w,h)
IMG_SIZE = 640               # model imgsz
DISPLAY_SIZE = (800, 450)    # display copy
PROCESS_EVERY_N = 1          # inference pacing
PIG_CONF_MIN = 0.40          # raise base pig confidence to reduce false positives
MIN_AREA_FRAC = 0.01         # ignore boxes smaller than this fraction of full-frame area (1% default)
ASPECT_RATIO_MIN = 0.5       # width/height lower bound for pigs
ASPECT_RATIO_MAX = 2.0       # width/height upper bound for pigs
CONFIRM_FRAMES = 2           # require detection to appear in N consecutive inference cycles
MAX_TRACK_LOST = 3           # allow up to N missed frames before deleting a track
APPLY_SHARPEN = True         # keep mild sharpening from previous script

CONF_ASF = 0.50
CONF_BEHAVIOR = 0.50

# -------------------------- Model paths (adjust if needed) --------------------------
MODEL_ASF_PATH = "/home/asfrotect/Downloads/best(ASF_MODEL)_.pt"
MODEL_BEHAVIOR_PATH = "/home/asfrotect/Downloads/best(BEHAVIOR_MODEL)_.pt"
MODEL_PIG_PATH = "/home/asfrotect/Downloads/pig_vs_non_pig.v3.pt"

print("Loading models (this may take a while)...")
pig_model = YOLO(MODEL_PIG_PATH)
asf_model = YOLO(MODEL_ASF_PATH)
behavior_model = YOLO(MODEL_BEHAVIOR_PATH)
print("Models loaded.")

# -------------------------- Shared state (unchanged) --------------------------
shared = {
    "latest_frame": None,
    "latest_results": [],   # this will contain only confirmed tracks (for display)
    "frame_ts": 0.0,
    "results_ts": 0.0
}
shared_lock = threading.Lock()
stop_event = threading.Event()

# -------------------------- Camera thread (video config + autofocus) --------------------------
class CameraThread(threading.Thread):
    def __init__(self, cap_size=(1280,720), framerate=30):
        super().__init__(daemon=True)
        self.picam2 = Picamera2()
        self.config = self.picam2.create_video_configuration(main={"format": "RGB888", "size": cap_size})
        self.picam2.configure(self.config)
        self.picam2.start()
        try:
            self.picam2.set_controls({"AfMode": controls.AfModeEnum.Continuous})
            print("Autofocus set: Continuous")
        except Exception as e:
            print("Autofocus enable failed:", e)
        self.stopped = False

    def run(self):
        while not stop_event.is_set():
            try:
                frame = self.picam2.capture_array()
                if frame is None:
                    time.sleep(0.005)
                    continue
                with shared_lock:
                    shared["latest_frame"] = frame
                    shared["frame_ts"] = time.time()
            except Exception as e:
                print("Camera capture error:", e)
                time.sleep(0.01)

    def stop(self):
        stop_event.set()
        try:
            self.picam2.stop()
        except Exception:
            pass

# -------------------------- Tracking utilities --------------------------
def iou(boxA, boxB):
    # boxes are (x1,y1,x2,y2)
    ax1, ay1, ax2, ay2 = boxA
    bx1, by1, bx2, by2 = boxB
    inter_x1 = max(ax1, bx1)
    inter_y1 = max(ay1, by1)
    inter_x2 = min(ax2, bx2)
    inter_y2 = min(ay2, by2)
    iw = max(0, inter_x2 - inter_x1)
    ih = max(0, inter_y2 - inter_y1)
    inter_area = iw * ih
    areaA = max(0, ax2 - ax1) * max(0, ay2 - ay1)
    areaB = max(0, bx2 - bx1) * max(0, by2 - by1)
    union = areaA + areaB - inter_area
    return inter_area / union if union > 0 else 0.0

# Simple track structure
class Track:
    def __init__(self, box, conf, behavior=None, lesions=None, parts=None):
        self.box = tuple(map(int, box))
        self.conf = float(conf)
        self.age = 1           # total frames seen
        self.seen = 1          # consecutive seen count
        self.lost = 0          # consecutive missed frames
        self.confirmed = False
        self.behavior_votes = [] if behavior is None else [behavior]
        self.lesion_votes = lesions[:] if lesions else []
        self.part_votes = parts[:] if parts else []

    def update(self, box, conf, behavior, lesions, parts):
        self.box = tuple(map(int, box))
        self.conf = (self.conf * (self.age - 1) + float(conf)) / self.age if self.age > 0 else float(conf)
        self.age += 1
        self.seen += 1
        self.lost = 0
        if behavior:
            self.behavior_votes.append(behavior)
        if lesions:
            self.lesion_votes.extend(lesions)
        if parts:
            self.part_votes.extend(parts)
        if self.seen >= CONFIRM_FRAMES:
            self.confirmed = True

    def mark_lost(self):
        self.lost += 1
        self.seen = 0

    def get_aggregate_behavior(self):
        # majority vote if any
        if not self.behavior_votes:
            return None
        vals = {}
        for b in self.behavior_votes:
            if b is None: continue
            k = b.lower()
            vals[k] = vals.get(k, 0) + 1
        if not vals:
            return None
        return max(vals.items(), key=lambda x: x[1])[0]

# -------------------------- Inference + tracking thread (main changes here) --------------------------
class InferenceThread(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True)
        self.tracks = []   # list of Track objects
        self.proc_count = 0

    def safe_box_coords(self, box):
        try:
            coords = list(map(int, box.xyxy[0]))
        except Exception:
            coords = [int(x) for x in box.xyxy]
        return coords

    def unsharp_mask(self, img):
        blurred = cv2.GaussianBlur(img, (0,0), sigmaX=3, sigmaY=3)
        sharpened = cv2.addWeighted(img, 1.3, blurred, -0.3, 0)
        return sharpened.astype("uint8")

    def filter_candidate(self, box, conf, frame_shape):
        # Apply base filters: confidence, area, aspect ratio
        h_frame, w_frame = frame_shape[:2]
        x1, y1, x2, y2 = map(int, box)
        w = max(1, x2 - x1)
        h = max(1, y2 - y1)
        area = w * h
        if conf < PIG_CONF_MIN:
            return False
        frame_area = w_frame * h_frame
        if area < (MIN_AREA_FRAC * frame_area):
            return False
        ar = (w / h) if h > 0 else 0
        if ar < ASPECT_RATIO_MIN or ar > ASPECT_RATIO_MAX:
            return False
        return True

    def match_and_update_tracks(self, detections):
        """
        detections: list of dict {'box':(x1,y1,x2,y2), 'conf':float, 'behavior':str, 'lesions':[], 'parts':[]}
        """
        matched = set()
        # IoU threshold to consider same object
        IOU_THRESH = 0.4
        # First, try to match each detection to existing tracks
        for d in detections:
            best_iou = 0.0
            best_idx = None
            for i, tr in enumerate(self.tracks):
                iouv = iou(d['box'], tr.box)
                if iouv > best_iou:
                    best_iou = iouv
                    best_idx = i
            if best_iou >= IOU_THRESH and best_idx is not None:
                # update matched track
                tr = self.tracks[best_idx]
                tr.update(d['box'], d['conf'], d.get('behavior'), d.get('lesions'), d.get('parts'))
                matched.add(best_idx)
            else:
                # create new track
                nt = Track(d['box'], d['conf'], behavior=d.get('behavior'), lesions=d.get('lesions'), parts=d.get('parts'))
                self.tracks.append(nt)

        # Mark unmatched tracks as lost
        for i, tr in enumerate(self.tracks):
            if i not in matched:
                tr.mark_lost()

        # Remove tracks lost for too long
        self.tracks = [tr for tr in self.tracks if tr.lost <= MAX_TRACK_LOST]

    def produce_confirmed_results(self):
        results = []
        for tr in self.tracks:
            if tr.confirmed:
                results.append({
                    "box": tr.box,
                    "conf": tr.conf,
                    "behavior": tr.get_aggregate_behavior(),
                    "lesions": tr.lesion_votes,   # raw aggregated boxes (may be many)
                    "parts": tr.part_votes
                })
        return results

    def run(self):
        global pig_model, asf_model, behavior_model
        while not stop_event.is_set():
            frame_copy = None
            with shared_lock:
                if shared["latest_frame"] is not None:
                    frame_copy = shared["latest_frame"].copy()
            if frame_copy is None:
                time.sleep(0.005)
                continue

            self.proc_count += 1
            if self.proc_count % PROCESS_EVERY_N != 0:
                continue

            # frame_copy is RGB888; convert to BGR for OpenCV ops
            frame_bgr = frame_copy[..., ::-1] if frame_copy.ndim == 3 and frame_copy.shape[2] == 3 else frame_copy
            frame_proc = frame_bgr.astype("uint8")
            if APPLY_SHARPEN:
                try:
                    frame_proc = self.unsharp_mask(frame_proc)
                except Exception:
                    pass
            # convert to RGB for model
            frame_rgb = frame_proc[..., ::-1]

            # ---------- Pig detection ----------
            try:
                pig_results = pig_model.predict(source=frame_rgb, conf=PIG_CONF_MIN, imgsz=IMG_SIZE, save=False, show=False, verbose=False)
            except Exception as e:
                print("Pig predict error:", e)
                time.sleep(0.01)
                continue

            # Collect candidate detections after applying per-box filters
            candidates = []
            for r in pig_results:
                for box in r.boxes:
                    try:
                        cls_id = int(box.cls[0].item())
                        conf = float(box.conf[0].item())
                    except Exception:
                        cls_id = int(box.cls)
                        conf = float(box.conf)
                    label = r.names.get(cls_id, str(cls_id)) if hasattr(r, "names") else str(cls_id)
                    if "pig" not in label.lower():
                        continue
                    coords = self.safe_box_coords(box)
                    if not self.filter_candidate(coords, conf, frame_rgb.shape):
                        continue
                    # clamp coords
                    x1, y1, x2, y2 = coords
                    h_frame, w_frame = frame_rgb.shape[:2]
                    x1, y1 = max(0, x1), max(0, y1)
                    x2, y2 = min(w_frame - 1, x2), min(h_frame - 1, y2)
                    pig_crop = frame_rgb[y1:y2, x1:x2]
                    if pig_crop.size == 0:
                        continue

                    # ASF detection and behavior detection executed but will only be attached to track if track becomes confirmed
                    lesion_boxes = []
                    part_boxes = []
                    try:
                        asf_results = asf_model.predict(source=pig_crop, conf=CONF_ASF, imgsz=IMG_SIZE, save=False, show=False, verbose=False)
                    except Exception as e:
                        asf_results = []
                    for ar in asf_results:
                        for a_box in ar.boxes:
                            try:
                                a_cls = int(a_box.cls[0].item())
                                a_conf = float(a_box.conf[0].item())
                            except Exception:
                                a_cls = int(a_box.cls)
                                a_conf = float(a_box.conf)
                            if a_conf < CONF_ASF:
                                continue
                            a_label = ar.names[a_cls]
                            ax1, ay1, ax2, ay2 = self.safe_box_coords(a_box)
                            ax1f, ay1f = ax1 + x1, ay1 + y1
                            ax2f, ay2f = ax2 + x1, ay2 + y1
                            if "lesion" in a_label.lower():
                                lesion_boxes.append((ax1f, ay1f, ax2f, ay2f))
                            elif a_label.lower() in ["ear", "leg", "nose", "tail"]:
                                part_boxes.append((ax1f, ay1f, ax2f, ay2f, a_label))

                    behavior_label = None
                    try:
                        behavior_results = behavior_model.predict(source=pig_crop, conf=CONF_BEHAVIOR, imgsz=IMG_SIZE, save=False, show=False, verbose=False)
                    except Exception:
                        behavior_results = []
                    for br in behavior_results:
                        for b_box in br.boxes:
                            try:
                                b_cls = int(b_box.cls[0].item())
                                b_conf = float(b_box.conf[0].item())
                            except Exception:
                                b_cls = int(b_box.cls)
                                b_conf = float(b_box.conf)
                            if b_conf >= CONF_BEHAVIOR:
                                behavior_label = br.names[b_cls]
                                break
                        if behavior_label:
                            break

                    candidates.append({
                        "box": (x1, y1, x2, y2),
                        "conf": conf,
                        "behavior": behavior_label,
                        "lesions": lesion_boxes,
                        "parts": part_boxes
                    })

            # Match candidates into tracks & produce confirmed results
            self.match_and_update_tracks(candidates)
            confirmed = self.produce_confirmed_results()

            # Write confirmed detections to shared results for display
            with shared_lock:
                shared["latest_results"] = confirmed
                shared["results_ts"] = time.time()

            # tiny sleep
            time.sleep(0.001)

# -------------------------- Display helpers (unchanged) --------------------------
def draw_label_small(img, box, label, color=(0,255,0)):
    x1, y1, x2, y2 = map(int, box)
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    cv2.putText(img, label, (x1, max(15, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)

def main():
    cam_thread = CameraThread(cap_size=CAPTURE_SIZE)
    infer_thread = InferenceThread()
    cam_thread.start()
    infer_thread.start()
    time.sleep(0.3)  # warm up

    try:
        while True:
            with shared_lock:
                frame = shared.get("latest_frame", None)
                results = list(shared.get("latest_results", []))
            if frame is None:
                time.sleep(0.005)
                continue

            frame_bgr = frame[..., ::-1] if frame.ndim == 3 and frame.shape[2] == 3 else frame
            disp = cv2.resize(frame_bgr, DISPLAY_SIZE, interpolation=cv2.INTER_LINEAR)

            cap_w, cap_h = CAPTURE_SIZE
            disp_w, disp_h = DISPLAY_SIZE
            sx = disp_w / cap_w
            sy = disp_h / cap_h

            # draw only confirmed tracks (results)
            for r in results:
                bx1, by1, bx2, by2 = r["box"]
                sb = (int(bx1 * sx), int(by1 * sy), int(bx2 * sx), int(by2 * sy))
                behavior = r.get("behavior", None)
                label = f"pig-{r['conf']:.2f}" + (f"-{behavior}" if behavior else "")
                draw_label_small(disp, sb, label, color=(0,255,0))
                for lesion in r.get("lesions", []):
                    lb = (int(lesion[0]*sx), int(lesion[1]*sy), int(lesion[2]*sx), int(lesion[3]*sy))
                    draw_label_small(disp, lb, "possible lesion", color=(0,0,255))
                for part in r.get("parts", []):
                    pb = (int(part[0]*sx), int(part[1]*sy), int(part[2]*sx), int(part[3]*sy))
                    draw_label_small(disp, pb, part[4], color=(255,128,0))

            cv2.putText(disp, f"CAP: {CAPTURE_SIZE[0]}x{CAPTURE_SIZE[1]} IMG:{IMG_SIZE}", (8, disp.shape[0]-8), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1)
            cv2.imshow("Pig + ASF + Behavior (robust detect)", disp)
            if cv2.waitKey(1) & 0xFF == 27:
                break
            time.sleep(0.001)

    except KeyboardInterrupt:
        pass
    finally:
        stop_event.set()
        try:
            cam_thread.stop()
        except Exception:
            pass
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
