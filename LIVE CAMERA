# fix_detect_RPI_LiveDet.py
# Run: python fix_detect_RPI_LiveDet.py
# Keeps smooth display. Fixes color-order, clears stale candidates, logs detection counts, relaxed-but-sane filters.

import time
import threading
import cv2
from ultralytics import YOLO
from picamera2 import Picamera2
from libcamera import controls
import numpy as np
import torch

# PyTorch weights fix (kept)
orig_torch_load = torch.load
def torch_load_wrapper(*args, **kwargs):
    kwargs["weights_only"] = False
    return orig_torch_load(*args, **kwargs)
torch.load = torch_load_wrapper

# -------------------- Tunables --------------------
CAPTURE_SIZE = (1280, 720)
IMG_SIZE = 640
DISPLAY_SIZE = (800, 450)
PROCESS_EVERY_N = 1

# Filters (relaxed but useful)
PIG_CONF_MIN = 0.25       # lower so we get detections
MIN_AREA_FRAC = 0.003     # 0.3% of frame area minimum (capture 1280x720 -> ~2760 px)
ASPECT_RATIO_MIN = 0.3
ASPECT_RATIO_MAX = 3.0
CONFIRM_FRAMES = 1
MAX_TRACK_LOST = 5

APPLY_SHARPEN = False     # turned off to avoid artifacts; enable if you want later

CONF_ASF = 0.50
CONF_BEHAVIOR = 0.50

SHOW_CANDIDATES = True

# Model paths (change if needed)
MODEL_ASF_PATH = "/home/asfrotect/Downloads/best(ASF_MODEL)_.pt"
MODEL_BEHAVIOR_PATH = "/home/asfrotect/Downloads/best(BEHAVIOR_MODEL)_.pt"
MODEL_PIG_PATH = "/home/asfrotect/Downloads/pig_vs_non_pig.v3.pt"

print("Loading YOLO models (may take a while)...")
pig_model = YOLO(MODEL_PIG_PATH)
asf_model = YOLO(MODEL_ASF_PATH)
behavior_model = YOLO(MODEL_BEHAVIOR_PATH)
print("Models loaded.")

# -------------------- Shared state --------------------
shared = {
    "latest_frame": None,
    "latest_results": [],    # confirmed tracks for display
    "latest_candidates": [], # candidate boxes for display
    "frame_ts": 0.0,
    "results_ts": 0.0
}
shared_lock = threading.Lock()
stop_event = threading.Event()

# -------------------- Camera thread --------------------
class CameraThread(threading.Thread):
    def __init__(self, cap_size=(1280,720)):
        super().__init__(daemon=True)
        self.picam2 = Picamera2()
        self.config = self.picam2.create_video_configuration(main={"format":"RGB888","size":cap_size})
        self.picam2.configure(self.config)
        self.picam2.start()
        try:
            self.picam2.set_controls({"AfMode": controls.AfModeEnum.Continuous})
            print("Autofocus: Continuous")
        except Exception as e:
            print("Autofocus enable failed:", e)
        self.stopped = False

    def run(self):
        while not stop_event.is_set():
            try:
                frame = self.picam2.capture_array()
                if frame is None:
                    time.sleep(0.005)
                    continue
                with shared_lock:
                    shared["latest_frame"] = frame
                    shared["frame_ts"] = time.time()
            except Exception as e:
                print("Camera capture error:", e)
                time.sleep(0.01)

    def stop(self):
        stop_event.set()
        try:
            self.picam2.stop()
        except Exception:
            pass

# -------------------- Utilities / Tracking --------------------
def iou(boxA, boxB):
    ax1, ay1, ax2, ay2 = boxA
    bx1, by1, bx2, by2 = boxB
    inter_x1 = max(ax1, bx1)
    inter_y1 = max(ay1, by1)
    inter_x2 = min(ax2, bx2)
    inter_y2 = min(ay2, by2)
    iw = max(0, inter_x2 - inter_x1)
    ih = max(0, inter_y2 - inter_y1)
    inter = iw * ih
    areaA = max(0, ax2 - ax1) * max(0, ay2 - ay1)
    areaB = max(0, bx2 - bx1) * max(0, by2 - by1)
    union = areaA + areaB - inter
    return inter / union if union > 0 else 0.0

class Track:
    def __init__(self, box, conf, behavior=None, lesions=None, parts=None):
        self.box = tuple(map(int, box))
        self.conf = float(conf)
        self.age = 1
        self.seen = 1
        self.lost = 0
        self.confirmed = False
        self.behavior_votes = [] if behavior is None else [behavior]
        self.lesion_votes = lesions[:] if lesions else []
        self.part_votes = parts[:] if parts else []

    def update(self, box, conf, behavior, lesions, parts):
        self.box = tuple(map(int, box))
        # running average for conf
        self.conf = (self.conf * (self.age - 1) + float(conf)) / self.age if self.age > 0 else float(conf)
        self.age += 1
        self.seen += 1
        self.lost = 0
        if behavior:
            self.behavior_votes.append(behavior)
        if lesions:
            self.lesion_votes.extend(lesions)
        if parts:
            self.part_votes.extend(parts)
        if self.seen >= CONFIRM_FRAMES:
            self.confirmed = True

    def mark_lost(self):
        self.lost += 1
        self.seen = 0

    def get_aggregate_behavior(self):
        if not self.behavior_votes:
            return None
        counts = {}
        for b in self.behavior_votes:
            if b is None: continue
            k = b.lower()
            counts[k] = counts.get(k, 0) + 1
        if not counts:
            return None
        return max(counts.items(), key=lambda x: x[1])[0]

# -------------------- Inference + Tracking thread --------------------
class InferenceThread(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True)
        self.tracks = []
        self.proc_count = 0

    def safe_box_coords(self, box):
        try:
            coords = list(map(int, box.xyxy[0]))
        except Exception:
            coords = [int(x) for x in box.xyxy]
        return coords

    def unsharp_mask(self, img):
        blurred = cv2.GaussianBlur(img, (0,0), sigmaX=3, sigmaY=3)
        sharpened = cv2.addWeighted(img, 1.2, blurred, -0.2, 0)
        return sharpened.astype("uint8")

    def filter_candidate(self, box, conf, frame_shape):
        h_frame, w_frame = frame_shape[:2]
        x1, y1, x2, y2 = map(int, box)
        w = max(1, x2 - x1)
        h = max(1, y2 - y1)
        area = w * h
        frame_area = w_frame * h_frame
        # relaxed filter: require very low threshold only
        if conf < (PIG_CONF_MIN * 0.5):
            return False
        if area < (MIN_AREA_FRAC * frame_area * 0.5):
            return False
        ar = (w / h) if h > 0 else 0
        if ar < (ASPECT_RATIO_MIN * 0.6) or ar > (ASPECT_RATIO_MAX * 1.4):
            return False
        return True

    def match_and_update_tracks(self, detections):
        matched = set()
        IOU_THRESH = 0.35
        for d in detections:
            best_iou = 0.0
            best_idx = None
            for i, tr in enumerate(self.tracks):
                iouv = iou(d['box'], tr.box)
                if iouv > best_iou:
                    best_iou = iouv
                    best_idx = i
            if best_iou >= IOU_THRESH and best_idx is not None:
                tr = self.tracks[best_idx]
                tr.update(d['box'], d['conf'], d.get('behavior'), d.get('lesions'), d.get('parts'))
                matched.add(best_idx)
            else:
                nt = Track(d['box'], d['conf'], behavior=d.get('behavior'), lesions=d.get('lesions'), parts=d.get('parts'))
                self.tracks.append(nt)
        for i, tr in enumerate(self.tracks):
            if i not in matched:
                tr.mark_lost()
        self.tracks = [tr for tr in self.tracks if tr.lost <= MAX_TRACK_LOST]

    def produce_confirmed_results(self):
        results = []
        for tr in self.tracks:
            if tr.confirmed:
                results.append({
                    "box": tr.box,
                    "conf": tr.conf,
                    "behavior": tr.get_aggregate_behavior(),
                    "lesions": tr.lesion_votes,
                    "parts": tr.part_votes
                })
        return results

    def run(self):
        global pig_model, asf_model, behavior_model
        while not stop_event.is_set():
            frame_copy = None
            with shared_lock:
                if shared["latest_frame"] is not None:
                    frame_copy = shared["latest_frame"].copy()
            if frame_copy is None:
                time.sleep(0.005)
                continue

            self.proc_count += 1
            if self.proc_count % PROCESS_EVERY_N != 0:
                continue

            # frame_copy is RGB888 from Picamera2
            # convert to BGR for OpenCV ops and pass BGR to model.predict (fixed color order)
            frame_bgr = frame_copy[..., ::-1] if frame_copy.ndim == 3 and frame_copy.shape[2] == 3 else frame_copy
            frame_proc = frame_bgr.astype("uint8")
            if APPLY_SHARPEN:
                try:
                    frame_proc = self.unsharp_mask(frame_proc)
                except Exception:
                    pass

            # IMPORTANT: pass BGR to model.predict to match earlier working path
            # (Ultralytics accepts numpy arrays in BGR OpenCV format)
            try:
                pig_results = pig_model.predict(source=frame_proc, conf=PIG_CONF_MIN, imgsz=IMG_SIZE, save=False, show=False, verbose=False)
            except Exception as e:
                print("Pig predict error:", e)
                time.sleep(0.01)
                continue

            candidates = []
            candidate_display = []

            # debug counters
            pig_boxes_total = 0

            for r in pig_results:
                for box in r.boxes:
                    pig_boxes_total += 1
                    try:
                        cls_id = int(box.cls[0].item())
                        conf = float(box.conf[0].item())
                    except Exception:
                        cls_id = int(box.cls)
                        conf = float(box.conf)
                    label = r.names.get(cls_id, str(cls_id)) if hasattr(r, "names") else str(cls_id)
                    if "pig" not in label.lower():
                        continue
                    coords = self.safe_box_coords(box)

                    # relaxed filter for candidates
                    if not self.filter_candidate(coords, conf, frame_proc.shape):
                        # still show as candidate (if toggled)
                        candidate_display.append((coords, conf))
                        continue

                    x1, y1, x2, y2 = coords
                    h_frame, w_frame = frame_proc.shape[:2]
                    x1, y1 = max(0, x1), max(0, y1)
                    x2, y2 = min(w_frame - 1, x2), min(h_frame - 1, y2)
                    pig_crop = frame_proc[y1:y2, x1:x2]
                    if pig_crop.size == 0:
                        continue

                    # ASF detection on pig_crop
                    lesion_boxes = []
                    part_boxes = []
                    try:
                        asf_results = asf_model.predict(source=pig_crop, conf=CONF_ASF, imgsz=IMG_SIZE, save=False, show=False, verbose=False)
                    except Exception:
                        asf_results = []
                    for ar in asf_results:
                        for a_box in ar.boxes:
                            try:
                                a_cls = int(a_box.cls[0].item())
                                a_conf = float(a_box.conf[0].item())
                            except Exception:
                                a_cls = int(a_box.cls)
                                a_conf = float(a_box.conf)
                            if a_conf < CONF_ASF:
                                continue
                            a_label = ar.names[a_cls]
                            ax1, ay1, ax2, ay2 = self.safe_box_coords(a_box)
                            ax1f, ay1f = ax1 + x1, ay1 + y1
                            ax2f, ay2f = ax2 + x1, ay2 + y1
                            if "lesion" in a_label.lower():
                                lesion_boxes.append((ax1f, ay1f, ax2f, ay2f))
                            elif a_label.lower() in ["ear", "leg", "nose", "tail"]:
                                part_boxes.append((ax1f, ay1f, ax2f, ay2f, a_label))

                    # Behavior detection
                    behavior_label = None
                    try:
                        behavior_results = behavior_model.predict(source=pig_crop, conf=CONF_BEHAVIOR, imgsz=IMG_SIZE, save=False, show=False, verbose=False)
                    except Exception:
                        behavior_results = []
                    for br in behavior_results:
                        for b_box in br.boxes:
                            try:
                                b_cls = int(b_box.cls[0].item())
                                b_conf = float(b_box.conf[0].item())
                            except Exception:
                                b_cls = int(b_box.cls)
                                b_conf = float(b_box.conf)
                            if b_conf >= CONF_BEHAVIOR:
                                behavior_label = br.names[b_cls]
                                break
                        if behavior_label:
                            break

                    cand = {
                        "box": (x1, y1, x2, y2),
                        "conf": conf,
                        "behavior": behavior_label,
                        "lesions": lesion_boxes,
                        "parts": part_boxes
                    }
                    candidates.append(cand)
                    candidate_display.append(((x1, y1, x2, y2), conf))

            # match candidates -> tracks
            self.match_and_update_tracks(candidates)
            confirmed = self.produce_confirmed_results()

            # DEBUG LOG: counts
            print(f"[infer] pig_box_candidates={pig_boxes_total} -> candidates={len(candidates)} confirmed_tracks={len(confirmed)}")

            # publish results (ensure we clear candidates when empty)
            with shared_lock:
                shared["latest_results"] = confirmed
                shared["latest_candidates"] = candidate_display if SHOW_CANDIDATES else []
                shared["results_ts"] = time.time()

            time.sleep(0.001)

    # Attach match_and_update_tracks and produce_confirmed_results from earlier definitions
    def match_and_update_tracks(self, detections):
        # method body reused from above (implement here)
        matched = set()
        IOU_THRESH = 0.35
        for d in detections:
            best_iou = 0.0
            best_idx = None
            for i, tr in enumerate(self.tracks):
                iouv = iou(d['box'], tr.box)
                if iouv > best_iou:
                    best_iou = iouv
                    best_idx = i
            if best_iou >= IOU_THRESH and best_idx is not None:
                tr = self.tracks[best_idx]
                tr.update(d['box'], d['conf'], d.get('behavior'), d.get('lesions'), d.get('parts'))
                matched.add(best_idx)
            else:
                nt = Track(d['box'], d['conf'], behavior=d.get('behavior'), lesions=d.get('lesions'), parts=d.get('parts'))
                self.tracks.append(nt)
        for i, tr in enumerate(self.tracks):
            if i not in matched:
                tr.mark_lost()
        self.tracks = [tr for tr in self.tracks if tr.lost <= MAX_TRACK_LOST]

    def produce_confirmed_results(self):
        return self.__class__.produce_confirmed_results(self)

# bind methods to class
InferenceThread.match_and_update_tracks = InferenceThread.match_and_update_tracks
InferenceThread.produce_confirmed_results = InferenceThread.produce_confirmed_results

# -------------------- Display helpers --------------------
def draw_label_small(img, box, label, color=(0,255,0), thickness=1):
    x1, y1, x2, y2 = map(int, box)
    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)
    cv2.putText(img, label, (x1, max(12, y1 - 4)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)

def draw_translucent_box(img, box, color=(0,255,255), alpha=0.18):
    x1, y1, x2, y2 = map(int, box)
    overlay = img.copy()
    cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 1)

# -------------------- Main display loop --------------------
def main():
    cam_thread = CameraThread(cap_size=CAPTURE_SIZE)
    infer_thread = InferenceThread()
    cam_thread.start()
    infer_thread.start()
    time.sleep(0.3)

    try:
        while True:
            with shared_lock:
                frame = shared.get("latest_frame", None)
                results = list(shared.get("latest_results", []))
                candidates = list(shared.get("latest_candidates", []))
            if frame is None:
                time.sleep(0.005)
                continue

            # frame is RGB888 -> convert to BGR for display (OpenCV)
            frame_bgr = frame[..., ::-1] if frame.ndim == 3 and frame.shape[2] == 3 else frame
            disp = cv2.resize(frame_bgr, DISPLAY_SIZE, interpolation=cv2.INTER_LINEAR)

            cap_w, cap_h = CAPTURE_SIZE
            disp_w, disp_h = DISPLAY_SIZE
            sx = disp_w / cap_w
            sy = disp_h / cap_h

            # draw candidates
            for (cbox, cconf) in candidates:
                cx1, cy1, cx2, cy2 = cbox
                sb = (int(cx1 * sx), int(cy1 * sy), int(cx2 * sx), int(cy2 * sy))
                draw_translucent_box(disp, sb, color=(255,200,0), alpha=0.15)
                cv2.putText(disp, f"cand:{cconf:.2f}", (sb[0], max(12, sb[1]-2)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,200,0), 1)

            # draw confirmed results
            for r in results:
                bx1, by1, bx2, by2 = r["box"]
                sb = (int(bx1 * sx), int(by1 * sy), int(bx2 * sx), int(by2 * sy))
                behavior = r.get("behavior", None)
                label = f"pig-{r['conf']:.2f}" + (f"-{behavior}" if behavior else "")
                draw_label_small(disp, sb, label, color=(0,255,0), thickness=2)
                for lesion in r.get("lesions", []):
                    lb = (int(lesion[0]*sx), int(lesion[1]*sy), int(lesion[2]*sx), int(lesion[3]*sy))
                    draw_label_small(disp, lb, "lesion", color=(0,0,255), thickness=1)
                for part in r.get("parts", []):
                    pb = (int(part[0]*sx), int(part[1]*sy), int(part[2]*sx), int(part[3]*sy))
                    draw_label_small(disp, pb, part[4], color=(255,128,0), thickness=1)

            cv2.putText(disp, f"CAP:{CAPTURE_SIZE[0]}x{CAPTURE_SIZE[1]} IMG:{IMG_SIZE}", (8, disp.shape[0]-8), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1)
            cv2.imshow("Pig + ASF + Behavior (fixed detect)", disp)
            if cv2.waitKey(1) & 0xFF == 27:
                break
            time.sleep(0.001)

    except KeyboardInterrupt:
        pass
    finally:
        stop_event.set()
        try:
            cam_thread.stop()
        except Exception:
            pass
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
